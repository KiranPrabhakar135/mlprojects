{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "KeXmdbJl9kCy",
    "colab_type": "code",
    "colab": {}
   },
   "outputs": [],
   "source": [
    "# The current method creates an instance of simple cartpole environment \n",
    "# In which the number of actions that the agent can take is either moving left or right\n",
    "# The reward +1 is given if the system remains upright\n",
    "def create_environment():\n",
    "  environment = gym.make('CartPole-v1')\n",
    "  return environment\n",
    "\n",
    "# The generate method accepts the policy that is used for implementing DQN. \n",
    "\n",
    "\n",
    "def generate_model(policy, env):\n",
    "  # In the current setting Multi Layer perceptron is used as policy.\n",
    "  # The other policies that is tried out is cnnPolicy. \n",
    "  model = DQN(policy, env, verbose=1, batch_size = 128, exploration_fraction = 0.2, exploration_final_eps=0.02)\n",
    "  # The total_timesteps parameter determines the number of samples that the model is trained for each iteration.\n",
    "  # In the Tom and Jerry example, the number of steps is (6000*10) i.e (number of episodes * steps taken in each episode)\n",
    "  # The log interval parameter determines the number of steps after which the log information should be displayed.\n",
    "  model.learn(total_timesteps=70000, log_interval  = 100)\n",
    "  # Pikels the trained model for future.\n",
    "  model.save(\"deepq_cartpole\")\n",
    "  \n",
    "# The blow method runs the instance of cartpole environment for\n",
    "# iterations_count(1000) times, and renders the environment at each step\n",
    "def perform_learning(iterations_count, mean_success_criteria_reward): \n",
    "  # Un Pikel the model and loads into memory.\n",
    "  model = DQN.load(\"deepq_cartpole\")\n",
    "\n",
    "  reward_sum = 0 # Accumulates the reward calculated for each action and is used to check the success criteria\n",
    "  for i in range(iterations_count):\n",
    "    # Resets the environment for current iteration and retruns initial observation.\n",
    "    obs = env.reset()\n",
    "    while True:\n",
    "      # will return the tuple of current action and next state\n",
    "      action, _states = model.predict(obs)\n",
    "      # Performs action and returns\n",
    "      # observation of the environment,\n",
    "      # rewared for the action taken\n",
    "      # done flag to indicate if the environment should be reset for next iteration or not\n",
    "      # info: for additional debugging.\n",
    "      obs, rewards, dones, info = env.step(action)\n",
    "      reward_sum += rewards\n",
    "      print(reward_sum)\n",
    "     \n",
    "      if(reward_sum >= mean_success_criteria_reward or dones): # checking success criteria for the curent \n",
    "        print(\"Reward obtained for the episode is: \" + str(reward_sum))\n",
    "        break\n",
    "\n",
    "    env.render()\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "HGlIy4AgmApe",
    "colab_type": "code",
    "colab": {}
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "\n",
    "from stable_baselines.deepq.policies import MlpPolicy\n",
    "from stable_baselines import DQN\n",
    "\n",
    "\n",
    "iterations_count = 100 # number of episodes the \n",
    "mean_success_criteria_reward = 197 # The success criteria provided by GYM\n",
    "env = create_environment() # Creates an environment\n",
    "generate_model(MlpPolicy, env)\n",
    "perform_learning(iterations_count, mean_success_criteria_reward)\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "Untitled2.ipynb",
   "version": "0.3.2",
   "provenance": [],
   "collapsed_sections": []
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
